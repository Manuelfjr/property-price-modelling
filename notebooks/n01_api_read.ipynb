{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f4a374",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52426a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "# basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import re\n",
    "import geopandas as gpd\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "import pgeocode\n",
    "import urllib\n",
    "from shapely.geometry import Point\n",
    "from itertools import chain\n",
    "from kedro.config import ConfigLoader\n",
    "from ppm.utils.external_get_data import (\n",
    "    find_regex_cep,\n",
    "    search_cep,\n",
    "    find_census_area_by_zip,\n",
    "    get_connection,\n",
    "    scrapping_zipimoveis,\n",
    "    find_cbg\n",
    ")\n",
    "from ppm.utils.readers import (\n",
    "    reader,\n",
    "    unzip_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389625ff",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "uf = 'PB'\n",
    "root = os.path.join(\n",
    "    '..', 'oos_n2', # data or oos\n",
    ")\n",
    "path_raw = os.path.join(\n",
    "    root, '01_raw'\n",
    ")\n",
    "path_intermediate = os.path.join(\n",
    "    root, '02_intermediate'\n",
    ")\n",
    "path_primary = os.path.join(\n",
    "    root, '03_primary'\n",
    ")\n",
    "root_brasilapi = 'https://brasilapi.com.br/api'\n",
    "root_zap = 'https://www.zapimoveis.com.br/'\n",
    "url_path_ibge = os.path.join(\n",
    "    root, 'ibge', 'uf', 'v1'\n",
    ")\n",
    "url_path_pix = os.path.join(\n",
    "    root, 'pix', 'v1', 'participants'\n",
    ")\n",
    "url_path_zapimoveis = os.path.join(\n",
    "    root_zap, 'venda/imoveis/pb+joao-pessoa/?pagina={}'\n",
    ")\n",
    "url_path_ibge = os.path.join(\n",
    "    root, 'ibge', 'municipios', 'v1', f'{uf}?providers=dados-abertos-br,gov,wikipedia'\n",
    ")\n",
    "path_census_data = os.path.join(\n",
    "    path_raw, 'PB_20171016'\n",
    ")\n",
    "path_census_data_csv = os.path.join(\n",
    "    path_census_data,\n",
    "    f'{uf}',\n",
    "    f'Base informaçoes setores2010 universo {uf}',\n",
    "    'CSV'\n",
    ")\n",
    "\n",
    "file_path_data_merged = os.path.join(\n",
    "    path_intermediate, 'scrapping_data_concat.csv'\n",
    ")\n",
    "file_path_data_shp = os.path.join(\n",
    "    path_raw, \n",
    "    'PB_Setores_2021',\n",
    "    'PB_Setores_2021.shp'\n",
    ")\n",
    "\n",
    "file_path_processed = os.path.join(\n",
    "    path_primary, \"data_processed.csv\"\n",
    ")\n",
    "file_path_data_input = os.path.join(\n",
    "    path_primary, \"data_input.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coefs(x: float) -> float:\n",
    "    return np.abs(x)/(sum(np.abs(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d0ead",
   "metadata": {},
   "source": [
    "# Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0beda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "if not os.path.exists(path_census_data):\n",
    "    unzip_file(\n",
    "        path_census_data+'.zip',\n",
    "        path_census_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb89e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_census_data = {\n",
    "    i.split(\".\")[0]: os.path.join(path_census_data_csv, i)for i in os.listdir(path_census_data_csv) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d119de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_census = {\n",
    "    i: reader(url) for i, url in file_path_census_data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shp = gpd.read_file(file_path_data_shp)\n",
    "data_shp.columns = data_shp.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d8982",
   "metadata": {},
   "source": [
    "# Concatenate census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58121873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (name, content) in enumerate(data_census.items()):\n",
    "    if idx == 0:\n",
    "        data_census_merged = content.copy()\n",
    "        data_census_merged.columns = data_census_merged.columns.str.lower()\n",
    "        data_census_merged.columns = [\"cod_setor\"]+[\n",
    "            '{}_{}'.format(i, name) for i in data_census_merged.columns[1:]\n",
    "        ]\n",
    "    else:\n",
    "        content.columns = content.columns.str.lower()\n",
    "        content.columns = [\"cod_setor\"]+[\n",
    "            '{}_{}'.format(i, name) for i in content.columns[1:]\n",
    "        ]\n",
    "        data_census_merged = data_census_merged.merge(\n",
    "            content,\n",
    "            on = [\"cod_setor\"],\n",
    "            suffixes = (\n",
    "                \"_{}\".format(name).lower(),\n",
    "                \"_{}\".format(name).lower()\n",
    "            )\n",
    "        )\n",
    "        data_census_merged.columns = data_census_merged.columns.str.lower()\n",
    "    if 'v999' in data_census_merged.columns:\n",
    "        print(data_census_merged.columns)\n",
    "data_census_merged.columns = data_census_merged.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_census_merged.drop(\n",
    "    data_census_merged.columns[data_census_merged.columns.str.contains('unnamed')],\n",
    "    axis = 1, inplace = True\n",
    ")\n",
    "data_census_merged = data_census_merged.loc[:, ~data_census_merged.columns.duplicated()]\n",
    "data_census_merged.rename(\n",
    "        columns = {\n",
    "            \"cod_setor\": \"cd_setor\"\n",
    "        }, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f9b40",
   "metadata": {},
   "source": [
    "# Filter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ebed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_joao_pessoa\n",
    "data_census_merged = data_census_merged[\n",
    "    data_census_merged.cd_setor.astype(\n",
    "        str\n",
    "    ).str.contains(\n",
    "        '2507507'\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47404cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_joao_pessoa\n",
    "data_shp = data_shp[\n",
    "    data_shp.cd_setor.astype(\n",
    "        str\n",
    "    ).str.contains(\n",
    "        '2507507'\n",
    "    )\n",
    "].copy()\n",
    "data_shp.rename(\n",
    "    columns = {\n",
    "        \"cod_setor\": \"cd_setor\"\n",
    "    }, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c6be8",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_init = 51\n",
    "n_pages_final = 52 # 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ac527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_results_scrapping = {}\n",
    "for i in tqdm(range(n_pages_init, n_pages_final + 1)):\n",
    "    data_results_scrapping[\n",
    "        f'page_n{i}'\n",
    "    ] = scrapping_zipimoveis(url_path_zapimoveis.format(i))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_final = {}\n",
    "errors = []\n",
    "for page, content in tqdm(data_results_scrapping.items()):\n",
    "    data_tmp = {}\n",
    "    for idx, i in enumerate(list(content.items())):\n",
    "        data_tmp[f'n{idx}'] = {}\n",
    "        data_tmp[f'n{idx}']['ID'] = i[0].replace('id-','')\n",
    "        data_tmp[f'n{idx}']['price'] = transform_float(i[1]['price'][0])\n",
    "        data_tmp[f'n{idx}']['local'] = i[1]['local']\n",
    "        for f_name, f_value in i[1]['features'].items():\n",
    "            data_tmp[f'n{idx}'][f_name] = replace_names(f_value)\n",
    "        data_tmp[f'n{idx}'] = pd.DataFrame(data_tmp[f'n{idx}'])\n",
    "    try:\n",
    "        content_final[page] = pd.concat(list(data_tmp.values()),\n",
    "                                        ignore_index = True)\n",
    "    except Exception as e:\n",
    "        errors.append((page, e))\n",
    "content_final = pd.concat(list(content_final.values()),\n",
    "                          ignore_index = True).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = [\n",
    "    'areas',\n",
    "    'bedrooms',\n",
    "    'parking-spaces',\n",
    "    'bathrooms'\n",
    "]\n",
    "string_rows = content_final[cols_selected].apply(lambda x: x.str.contains('-', na = False))\n",
    "\n",
    "for i in cols_selected:\n",
    "    values_changed = content_final[\n",
    "        string_rows[i]\n",
    "    ][i].apply(\n",
    "        lambda x: (int(x.split(\"-\")[0])+int(x.split(\"-\")[1]))/2\n",
    "    )\n",
    "    content_final.loc[values_changed.index, i] = values_changed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a5f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ceps = []\n",
    "for i in tqdm(content_final['local']):\n",
    "    try:\n",
    "        ceps.append( search_cep(i)[1] )\n",
    "    except:\n",
    "        ceps.append( np.nan )\n",
    "content_final['cep'] = ceps\n",
    "content_final['cep'] = content_final['cep'].fillna(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8432f",
   "metadata": {},
   "source": [
    "## Save scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_final.to_csv(file_path_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154fb63",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"bathrooms\"\n",
    "]\n",
    "which_rows_to_drop = [\n",
    "    'areas', \n",
    "    'bedrooms',\n",
    "    'parking-spaces'\n",
    "]\n",
    "id_tag = [\n",
    "    \"ID\"\n",
    "]\n",
    "content_final_process = content_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_final_process.price = content_final_process.price.apply(convert_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = []\n",
    "longs = []\n",
    "cbg = []\n",
    "nomi = pgeocode.Nominatim('br')\n",
    "for local in  tqdm(content_final_process.local):\n",
    "    if local!=None:\n",
    "        address = local + f', {uf}, Brasil'\n",
    "        try:\n",
    "\n",
    "            geolocator = Nominatim(user_agent=\"geolocalização\")\n",
    "            location = geolocator.geocode(address)\n",
    "\n",
    "            lats.append(float(location.latitude))\n",
    "            longs.append(float(location.longitude))\n",
    "\n",
    "        except:\n",
    "            lats.append(np.nan)\n",
    "            longs.append(np.nan)\n",
    "\n",
    "    else:\n",
    "        lats.append(np.nan)\n",
    "        longs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_final_process['lats'] = lats\n",
    "content_final_process['longs'] = longs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fbfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar os objetos de ponto com as latitudes e longitudes\n",
    "pontos = [Point(xy) for xy in zip(content_final_process['longs'], content_final_process['lats'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c76fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbgs = find_cbg(data_shp, pontos)\n",
    "content_final_process['cd_setor'] = cbgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2908b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order columns\n",
    "id_columns = [\n",
    "    \"ID\",\n",
    "    \"cd_setor\"\n",
    "]\n",
    "content_final_process = content_final_process[id_columns+[i for i in content_final_process.columns if i not in id_columns]]\n",
    "content_processed = content_final_process.dropna(subset = [\"cd_setor\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed.cd_setor = content_processed.cd_setor.astype(str)\n",
    "data_census_merged.cd_setor = data_census_merged.cd_setor.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = content_processed.merge(\n",
    "    data_census_merged, \n",
    "    on = [\"cd_setor\"],\n",
    "    how = \"left\"\n",
    ")\n",
    "data_merged = data_merged.merge(\n",
    "    data_shp[[\"cd_setor\",\"nm_sit\"]],\n",
    "    on = [\"cd_setor\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "data_merged.insert(\n",
    "    4,\n",
    "    \"bairro\",\n",
    "    data_merged.local.apply(\n",
    "        lambda x: x.split(\", \")[-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff12617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.columns = data_merged.columns.str.replace(\" \",\"_\")\n",
    "columns_na = data_merged.isna().sum()[data_merged.isna().sum()!=0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4535c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_bairros = {}\n",
    "errors = []\n",
    "for bairro in tqdm(data_merged.bairro.unique()):\n",
    "    data_bairros[bairro] = data_merged[\n",
    "        data_merged.bairro.str.contains(bairro)\n",
    "    ].reset_index(drop = True)\n",
    "    for na_col in tqdm(columns_na):\n",
    "        #print(na_col)\n",
    "        try:\n",
    "            data_bairros[bairro][na_col] = data_bairros[\n",
    "                bairro\n",
    "            ][\n",
    "                na_col\n",
    "            ].astype(\n",
    "                float\n",
    "            )\n",
    "            average = data_bairros[bairro][na_col].mean()\n",
    "            data_bairros[bairro][na_col] = data_bairros[\n",
    "                bairro\n",
    "            ][\n",
    "                na_col\n",
    "            ].fillna(\n",
    "                average\n",
    "            )\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "            continue\n",
    "data_merged_not_nan = pd.concat(list(data_bairros.values()), ignore_index = True)\n",
    "data_merged_not_nan = data_merged_not_nan.dropna().reset_index(drop = True)\n",
    "data_merged_not_nan = data_merged_not_nan[\n",
    "    ~data_merged_not_nan['ID'].duplicated()\n",
    "].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_not_nan.drop(\n",
    "    data_merged_not_nan.columns[data_merged_not_nan.columns.str.contains(\"situacao\")],\n",
    "    axis = 1,\n",
    "    inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedce512",
   "metadata": {},
   "source": [
    "## Save data processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed5ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_merged_not_nan.to_csv(file_path_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26998107",
   "metadata": {},
   "source": [
    "# Drop columns with problemns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d169f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cods_cols = list(data_merged_not_nan.columns[data_merged_not_nan.columns.str.contains(\"cod\")])\n",
    "other_cols = [\n",
    "    \"cep\",\n",
    "    \"lats\",\n",
    "    \"longs\",\n",
    "    \"local\",\n",
    "    \"nm_sit\"\n",
    "]\n",
    "all_columns = cods_cols + other_cols\n",
    "data_merged_not_nan.drop(all_columns,\n",
    "                         axis = 1, \n",
    "                         inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d47ee",
   "metadata": {},
   "source": [
    "# Categorical columns process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"bairro\"\n",
    "]\n",
    "cat_dict = {\n",
    "    \n",
    "}\n",
    "for col in cat_cols:\n",
    "    for idx, i in enumerate(data_merged_not_nan[col].unique()):\n",
    "        cat_dict[col] = {\n",
    "            i: int(idx)\n",
    "        }\n",
    "        data_merged_not_nan[col] = data_merged_not_nan[col].replace(cat_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_merged_not_nan.copy()\n",
    "data_final = data_final.replace(\"X\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64cb422",
   "metadata": {},
   "source": [
    "## Process column with problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc81eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in data_final.columns[data_final.columns.str.contains(\"nome\")]:\n",
    "    cat_values = {\n",
    "        i: idx for idx, i in enumerate(data_final[cat].unique())\n",
    "    }\n",
    "    data_final[cat] = data_final[cat].replace(cat_values)\n",
    "    \n",
    "for pro_col in data_final.columns:\n",
    "    try:\n",
    "        data_final[pro_col] = data_final[pro_col].astype(float)\n",
    "    except:\n",
    "        data_final[pro_col] = data_final[pro_col].astype(str).str.replace(\",\",\".\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20fb57",
   "metadata": {},
   "source": [
    "## Drop bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9273e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_final.drop(\"bathrooms\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d254cb",
   "metadata": {},
   "source": [
    "# Save dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44959e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv(file_path_data_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
